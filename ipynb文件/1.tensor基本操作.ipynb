{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe6fb930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a1b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cace8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a499317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_HAS_OPS',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_image_backend',\n",
       " '_internally_replaced_utils',\n",
       " '_is_tracing',\n",
       " '_video_backend',\n",
       " 'datasets',\n",
       " 'extension',\n",
       " 'get_image_backend',\n",
       " 'get_video_backend',\n",
       " 'io',\n",
       " 'models',\n",
       " 'ops',\n",
       " 'os',\n",
       " 'set_image_backend',\n",
       " 'set_video_backend',\n",
       " 'torch',\n",
       " 'transforms',\n",
       " 'utils',\n",
       " 'version',\n",
       " 'warnings']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(torchvision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c781068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd26e293",
   "metadata": {},
   "source": [
    "`torch.empty(5, 3)` 会创建一个形状为 (5, 3) 的张量，但不会初始化其内部数值，因此打印出的结果是内存中当前存在的随机垃圾值（每次运行结果可能不同）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe8ff473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7231, 0.0619, 0.5011],\n",
      "        [0.0921, 0.4521, 0.5870],\n",
      "        [0.3273, 0.7985, 0.8557],\n",
      "        [0.0207, 0.4971, 0.9601],\n",
      "        [0.6301, 0.2233, 0.6446]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2b5355",
   "metadata": {},
   "source": [
    "torch.rand(5, 3)：生成一个维度为 (5, 3) 的张量，其中每个元素都是从区间 [0, 1) 中随机采样的浮点数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f770eefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e919d09",
   "metadata": {},
   "source": [
    "`dtype=torch.long`：指定张量的数据类型为 int64（长整型），这是 PyTorch 中常用的整数类型（尤其适用于标签、索引等场景）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec58e4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2533a8c3",
   "metadata": {},
   "source": [
    "也可以自主生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed7619e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 0.6478, -0.0959,  0.4177],\n",
      "        [ 0.8985, -0.1680,  1.1329],\n",
      "        [-0.5323,  2.0668, -1.7331],\n",
      "        [ 0.2527, -0.0564, -1.1100],\n",
      "        [ 0.0308, -0.5756,  0.5339]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.float64) # x.new_ones(5, 3)：基于原张量 x 的设备（CPU/GPU）创建一个新的全一张量，形状为 (5, 3)（5 行 3 列）。\n",
    "print(x)\n",
    "x = torch.randn_like(x, dtype=torch.float) # 指定新的数据类型\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef1603",
   "metadata": {},
   "source": [
    "`torch.randn_like(x)`：生成一个与 x 形状完全相同的新张量，元素值从标准正态分布（N (0,1)）中随机采样。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1088d1",
   "metadata": {},
   "source": [
    "我们可以通过 `shape` 或者 `size()` 来获取 `Tensor` 的形状:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0daacfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5,3)\n",
    "print(x.size())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fabd6e4",
   "metadata": {},
   "source": [
    "常见的`Tensor` 函数\n",
    "\n",
    "`Tensor(*sizes)` 基础构造函数\n",
    "\n",
    "`tensor(data,)` 类似np.array的构造函数\n",
    "\n",
    "`ones(*sizes)` 全1Tensor\n",
    "\n",
    "`zeros(*sizes)` 全0Tensor\n",
    "\n",
    "`eye(*sizes)` 对⻆线为1，其他为0\n",
    "\n",
    "`arange(s,e,step)` 从s到e，步⻓为step\n",
    "\n",
    "`linspace(s,e,steps)` 从s到e，均匀切分成steps份\n",
    "\n",
    "`rand`/`randn(*sizes)` 均匀/标准分布\n",
    "\n",
    "`normal(mean,std)`/`uniform(from,to)` 正态分布/均匀分布\n",
    "\n",
    "`randperm(m)` 随机排列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0679b6",
   "metadata": {},
   "source": [
    "在深度学习中，我们通常会频繁地对数据进⾏操作。作为动⼿学深度学习的基础，本节将介绍如何对内\n",
    "存中的数据进⾏操作。\n",
    "\n",
    "`Tensor` 和`NumPy`的多维数组⾮常类似。然⽽， `Tensor` 提供GPU计算和⾃动求梯度等更多功能，这\n",
    "些使 `Tensor` 更加适合深度学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ed76ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6883, 0.4681, 1.4635],\n",
      "        [0.9395, 1.1340, 0.8437],\n",
      "        [0.6980, 0.8644, 1.1809],\n",
      "        [1.3395, 0.7018, 1.1553],\n",
      "        [1.5251, 1.2684, 0.6868]])\n",
      "tensor([[1.6883, 0.4681, 1.4635],\n",
      "        [0.9395, 1.1340, 0.8437],\n",
      "        [0.6980, 0.8644, 1.1809],\n",
      "        [1.3395, 0.7018, 1.1553],\n",
      "        [1.5251, 1.2684, 0.6868]])\n",
      "tensor([[1.6883, 0.4681, 1.4635],\n",
      "        [0.9395, 1.1340, 0.8437],\n",
      "        [0.6980, 0.8644, 1.1809],\n",
      "        [1.3395, 0.7018, 1.1553],\n",
      "        [1.5251, 1.2684, 0.6868]])\n",
      "tensor([[1.6883, 0.4681, 1.4635],\n",
      "        [0.9395, 1.1340, 0.8437],\n",
      "        [0.6980, 0.8644, 1.1809],\n",
      "        [1.3395, 0.7018, 1.1553],\n",
      "        [1.5251, 1.2684, 0.6868]])\n"
     ]
    }
   ],
   "source": [
    "# 加法形式1\n",
    "y = torch.rand(5,3)\n",
    "print(x+y)\n",
    "\n",
    "# 加法形式2\n",
    "print(torch.add(x,y))\n",
    "\n",
    "result = torch.empty(5,3)\n",
    "torch.add(x,y,out= result)\n",
    "print(result) \n",
    "\n",
    "# 加法形式3\n",
    "y.add_(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986d49ca",
   "metadata": {},
   "source": [
    "通过 `out=result` 参数，将加法结果直接写入到预先创建的 `result` 张量中，覆盖其原始的未初始化值；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2555b3b6",
   "metadata": {},
   "source": [
    "索引：类似`Numpy`的索引需要注意的是：索引出来的结果与\n",
    "原数据共享内存，也即修改⼀个，另⼀个会跟着修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9c7c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.8244, 1.1842, 1.6296])\n",
      "tensor([1.8244, 1.1842, 1.6296])\n"
     ]
    }
   ],
   "source": [
    "y = x[0,:]\n",
    "y +=1\n",
    "print(y)\n",
    "print(x[0,:]) # 索引出来的结果与原数据共享内存，修改一个另一个也跟着修改"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e685f",
   "metadata": {},
   "source": [
    "除了常⽤的索引选择数据之外，PyTorch还提供了⼀些⾼级的选择函数:\n",
    "\n",
    "`index_select(input, dim, index)` 在指定维度dim上选取，⽐如选取某些⾏、某些列\n",
    "\n",
    "`masked_select(input, mask)` 例⼦如上，a[a>0]，使⽤ByteTensor进⾏选取\n",
    "\n",
    "`non_zero(input)` ⾮0元素的下标\n",
    "\n",
    "`gather(input, dim, index)` 根据index，在dim维度上选取数据，输出的size与index⼀样\n",
    "\n",
    "这⾥不详细介绍，⽤到了再查官⽅⽂档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a948cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) torch.Size([15]) torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# 改变形状，类似numpy 的 reshape\n",
    "y = x.view(15)\n",
    "z = x.view(-1, 5) # -1所指的维度可以根据其他维度的值推出来\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c44324",
   "metadata": {},
   "source": [
    "用 `view()` 改变 `Tensor` 的形状与 `numpy` 中的 `reshape()`类似\n",
    "\n",
    "注意 `view()` 返回的新`tensor`与源`tensor`共享内存（其实是同⼀个`tensor`），也即更改其中的⼀个，另\n",
    "外⼀个也会跟着改变。(顾名思义，`view`仅仅是改变了对这个张量的观察⻆度)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b13f2bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.8244, 2.1842, 2.6296],\n",
      "        [1.6415, 1.4235, 1.8173],\n",
      "        [1.0322, 1.4709, 1.9113],\n",
      "        [1.5678, 1.3300, 1.1745],\n",
      "        [1.5515, 1.7992, 1.4200]])\n",
      "tensor([2.8244, 2.1842, 2.6296, 1.6415, 1.4235, 1.8173, 1.0322, 1.4709, 1.9113,\n",
      "        1.5678, 1.3300, 1.1745, 1.5515, 1.7992, 1.4200])\n"
     ]
    }
   ],
   "source": [
    "x += 1\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fabbcb",
   "metadata": {},
   "source": [
    "但当我们需要一个真正的新副本该怎么办？\n",
    "\n",
    "用`clone` 创造⼀个副本然后再使⽤ `view`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2423703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8244, 1.1842, 1.6296],\n",
      "        [0.6415, 0.4235, 0.8173],\n",
      "        [0.0322, 0.4709, 0.9113],\n",
      "        [0.5678, 0.3300, 0.1745],\n",
      "        [0.5515, 0.7992, 0.4200]])\n",
      "tensor([2.8244, 2.1842, 2.6296, 1.6415, 1.4235, 1.8173, 1.0322, 1.4709, 1.9113,\n",
      "        1.5678, 1.3300, 1.1745, 1.5515, 1.7992, 1.4200])\n"
     ]
    }
   ],
   "source": [
    "x_cp = x.clone().view(15)\n",
    "x -= 1\n",
    "print(x)\n",
    "print(x_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02a9b2a",
   "metadata": {},
   "source": [
    "另外⼀个常⽤的函数就是 `item()` , 它可以将⼀个标量 `Tensor` 转换成⼀个`Python number`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0faf6e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3042])\n",
      "0.3041820228099823\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa65685b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fd46dba",
   "metadata": {},
   "source": [
    "线性函数：另外，PyTorch还⽀持⼀些线性函数，这⾥提⼀下，免得⽤起来的时候⾃⼰造轮⼦，具体⽤法参考官⽅\n",
    "⽂档。如下表所示：\n",
    "\n",
    "`trace` 对⻆线元素之和(矩阵的迹)\n",
    "\n",
    "`diag` 对⻆线元素\n",
    "\n",
    "`triu`/`tril` 矩阵的上三⻆/下三⻆，可指定偏移量\n",
    "\n",
    "`mm`/`bmm` 矩阵乘法，batch的矩阵乘法\n",
    "\n",
    "`addmm`/`addbmm`/`addmv`/`addr`/`badbmm`.. 矩阵运算\n",
    "\n",
    "`t` 转置\n",
    "\n",
    "`dot`/`cross` 内积/外积\n",
    "\n",
    "`inverse` 求逆矩阵\n",
    "\n",
    "`svd` 奇异值分解\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733a251a",
   "metadata": {},
   "source": [
    "前⾯我们看到如何对两个形状相同的 Tensor 做按元素运算。当对两个形状不同的 Tensor 按元素运算\n",
    "时，可能会触发⼴播（broadcasting）机制：先适当复制元素使这两个 Tensor 形状相同后再按元素\n",
    "运算。例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a829fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1,3).view(1,2)\n",
    "print(x)\n",
    "y = torch.arange(1,4).view(3,1)\n",
    "print(y)\n",
    "print(x+y)\n",
    "#由于 x 和 y 分别是1⾏2列和3⾏1列的矩阵，如果要计算 x + y ，那么 x 中第⼀⾏的2个元素被⼴播（复制）到了第⼆⾏和第三⾏，⽽ y 中第⼀列的3个元素被⼴播（复制）到了第⼆列。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0408c3d",
   "metadata": {},
   "source": [
    "运算内存开销：前面说了，索引，`view`是不会开辟新内存的，而像 `y = x+y` 这样的运算是会新开内存的，然后将`y` 指向新内存，下面使用`python`演示这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6837cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,2])\n",
    "y = torch.tensor([3,4])\n",
    "id_before = id(y)\n",
    "y = y+x\n",
    "print(id(y) == id_before) #先将y的内存存入 id_before 变量中，更改 y 的值，查看两个 y 的内存是否相同\n",
    "\n",
    "#False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b2dda2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 6])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 如果想置顶结果道原来 y 的内存，该如何操作呢？\n",
    "\n",
    "x = torch.tensor([1,2])\n",
    "y = torch.tensor([3,4])\n",
    "id_before = id(y)\n",
    "y[:] = y+x\n",
    "print(y[:])\n",
    "print(id(y)== id_before)\n",
    "# 这里我们把 x + y 的结果通过 [:] 写进 y 对应的内存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d966bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y += x\n",
    "print(id_before == id(y))\n",
    "\n",
    "# 我们发现如果使用 y+=x 则不会生成新的内存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14638372",
   "metadata": {},
   "source": [
    "**Tensor 与 Numpy 的相互转换**\n",
    "\n",
    "⽤ `numpy()` 和 `from_numpy()` 将 `Tensor` 和`NumPy`中的数组相互转换。但是需要注意的⼀\n",
    "点是： 这两个函数所产⽣的的 `Tensor` 和`NumPy`中的数组共享相同的内存（所以他们之间的转换很\n",
    "快），改变其中⼀个时另⼀个也会改变\n",
    "\n",
    "有⼀个常⽤的将`NumPy`中的`array`转换成 `Tensor` 的⽅法就是 `torch.tensor()` , 需要注意的\n",
    "是，此⽅法总是会进⾏数据拷⻉（就会消耗更多的时间和空间），所以返回的 `Tensor` 和原来的数\n",
    "据不再共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81226c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n",
      "tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(a,b)\n",
    "\n",
    "a+=1\n",
    "print(a,b)\n",
    "b+=1\n",
    "print(a,b) # 改变 a ,b 中的任意一项都会导致两项发生改变"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2b112",
   "metadata": {},
   "source": [
    "`Numpy` 数组转 `Tensor`\n",
    "\n",
    "使用`from_numpy()` 将 `Numpy` 数组转换成 `Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9419b68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a,b)\n",
    "\n",
    "a+=1\n",
    "print(a,b)\n",
    "b+=1\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd3aef0",
   "metadata": {},
   "source": [
    "此外上⾯提到还有⼀个常⽤的⽅法就是直接⽤ `torch.tensor()` 将`NumPy`数组转换成 `Tensor` ，需要\n",
    "注意的是该⽅法总是会进⾏数据拷⻉，返回的 `Tensor` 和原来的数据不再共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4dd6d938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 4. 4. 4. 4.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "c = torch.tensor(a)\n",
    "a += 1\n",
    "print(a,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fee4af",
   "metadata": {},
   "source": [
    "**自动求梯度**\n",
    "\n",
    "如果将其属性 .requires_grad 设置为 True ，它将开始追踪(track)在其上的所有操作（这样就可以利⽤链式法则进⾏梯度传播了）。完成计算后，可以调⽤ .backward() 来完成所有梯度计算。此 Tensor 的梯度将累积到 .grad 属性中。\n",
    "\n",
    "如果不想要被继续追踪，可以调⽤ `.detach()` 将其从追踪记录中分离出来，这样就可以防⽌将来的计算被追踪，这样梯度就传不过去了。此外，还可以⽤ `with torch.``no_grad()` 将不想被追踪的操作代码块包裹起来，这种⽅法在评估模型的时候很常⽤，因为在评估模型时，我们并不需要计算可训练参数（ `requires_grad=True` ）的梯度。\n",
    "\n",
    "`Function` 是另外⼀个重要的类。 `Tensor` 和 `Function` 互相结合就可以构建⼀个记录有整个计算过程的有向⽆环图（DAG）。每个` Tensor` 都有⼀个 `.grad_fn` 属性，该属性即创建该 `Tensor` 的`Function` , 就是说该`Tensor` 是不是通过某些运算得到的，若是，则 `grad_fn` 返回⼀个与这些运算相关的对象，否则是`None`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9f9f4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2,requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22fb3065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x7fe0287bdb70>\n"
     ]
    }
   ],
   "source": [
    "# 加入运算操作\n",
    "y = x+2\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de5646",
   "metadata": {},
   "source": [
    "x 是直接创建的，所以它没有 `grad_fn`，而y 是通过一个加法操作创建的，所以它有一个为 `<AddBackward>`的`grad_fn`\n",
    "\n",
    "像 x 这种直接创建的称为叶子节点，叶子节点对应的 `grad_fn`是 `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d609b633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "print(x.is_leaf,y.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4872172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 运行复杂度运算操作\n",
    "\n",
    "z = y*y*3\n",
    "out = z.mean()\n",
    "print(z,out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2f329",
   "metadata": {},
   "source": [
    "tensor([[27., 27.],\n",
    "        [27., 27.]], grad_fn=<MulBackward0>) 是第一个张量\n",
    "`grad_fn=<MulBackward0>` 表示这个张量是通过乘法操作（* 或 torch.mul()）生成的，并且开启了梯度追踪（因为存在 grad_fn 属性）。\n",
    "\n",
    "\n",
    "\n",
    "tensor(27., grad_fn=<MeanBackward0>) 是第二个张量\n",
    "`grad_fn=<MeanBackward0>` 表示这个标量是通过求均值操作（.mean()）生成的，同样开启了梯度追踪\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "26074edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x7fe0287e60b8>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2) # 缺失情况下默认 requires_grad = False\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad) # False\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad) # True\n",
    "\n",
    "b = (a*a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a723ae45",
   "metadata": {},
   "source": [
    "我们令 $\\text{out}$ 为 $o$，因为  \n",
    "\n",
    "$$\n",
    "o = \\frac{1}{4} \\sum_{i=1}^{4} z_i = \\frac{1}{4} \\sum_{i=1}^{4} 3(x_i + 2)^2\n",
    "$$  \n",
    "\n",
    "所以  \n",
    "\n",
    "$$\n",
    "\\left. \\frac{\\partial o}{\\partial x_i} \\right|_{x_i=1} = \\frac{9}{2} = 4.5\n",
    "$$\n",
    "\n",
    "`out.backward()`会沿计算图反向传播，自动计算 `out` 对所有上游可训练张量（`requires_grad=True`）的梯度，并将结果存入这些张量的 `.grad` 属性中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d5ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea040675",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e28dc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad) # 结果正确"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be764ce8",
   "metadata": {},
   "source": [
    "量都为向量的函数 $\\vec{y} = f(\\vec{x})$，那么 $\\vec{y}$ 关于 $\\vec{x}$ 的梯度就是一个**雅可比矩阵（Jacobian matrix）**：  \n",
    "\n",
    "$$\n",
    "J = \\begin{pmatrix} \n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\cdots & \\frac{\\partial y_m}{\\partial x_n} \n",
    "\\end{pmatrix}\n",
    "$$  \n",
    "\n",
    "而 `torch.autograd` 这个包就是用来计算一些雅可比矩阵的乘积的。例如，如果 $v$ 是一个标量函数 $l = g(\\vec{y})$ 的梯度：  \n",
    "\n",
    "$$\n",
    "v = \\begin{pmatrix} \n",
    "\\frac{\\partial l}{\\partial y_1} & \\cdots & \\frac{\\partial l}{\\partial y_m} \n",
    "\\end{pmatrix}\n",
    "$$  \n",
    "\n",
    "那么根据**链式法则**，$l$ 关于 $\\vec{x}$ 的雅可比矩阵就为：  \n",
    "\n",
    "$$\n",
    "vJ = \\begin{pmatrix} \n",
    "\\frac{\\partial l}{\\partial y_1} & \\cdots & \\frac{\\partial l}{\\partial y_m} \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} \n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\cdots & \\frac{\\partial y_m}{\\partial x_n} \n",
    "\\end{pmatrix} \n",
    "= \\begin{pmatrix} \n",
    "\\frac{\\partial l}{\\partial x_1} & \\cdots & \\frac{\\partial l}{\\partial x_n} \n",
    "\\end{pmatrix}\n",
    "$$  \n",
    "\n",
    "`grad`在反向传播过程中是累加的，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以在反向传播之前需要把梯度清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27b72e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.5000, 6.5000],\n",
      "        [6.5000, 6.5000]])\n"
     ]
    }
   ],
   "source": [
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "# 1+4.5 = 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "85094d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "out3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c66d5c",
   "metadata": {},
   "source": [
    "不允许张量对张量求导，只允许标量对张量求导，求导结果是和⾃变量同形的张量。所以必要时我们要把张量通过将所有张量的元素加权求和的⽅式转换为标量，举个例⼦，假设 y 由 ⾃ 变 量 x 计 算 ⽽ 来 ， w 是 和 y 同 形 的 张 量 ，则 `y.backward(w)` 的含义是：先计算 `l = torch.sum(y * w)` ，则 `l` 是个标量，然后求 `l` 对⾃变量 `x` 的导数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ca3bc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1170, 1.5816, 0.5540], requires_grad=True)\n",
      "tensor([2.2339, 3.1631, 1.1081])\n"
     ]
    }
   ],
   "source": [
    "# 张量对张量求导：\n",
    "x = torch.randn(3,requires_grad=True)\n",
    "print(x)\n",
    "y = x**2\n",
    "y.backward(torch.ones(3))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4805b1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n",
      "tensor(18.)\n",
      "tensor(34.)\n"
     ]
    }
   ],
   "source": [
    "# 再来一个\n",
    "x1 = torch.tensor(1,requires_grad=True,dtype=torch.float)\n",
    "x2 = torch.tensor(2,requires_grad=True,dtype=torch.float)\n",
    "x3 = torch.tensor(3,requires_grad=True,dtype=torch.float)\n",
    "y = torch.randn(3)\n",
    "y[0]=x1**2+2*x2+x3 # define each vector function\n",
    "y[1]=x1+x2**3+x3**2\n",
    "y[2]=2*x1+x2**2+x3**3\n",
    "y.backward(torch.ones(3))\n",
    "print(x1.grad)\n",
    "print(x2.grad)\n",
    "print(x3.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00491468",
   "metadata": {},
   "source": [
    "上面代码中 Jacobian 矩阵为:  \n",
    "\n",
    "$$\n",
    "J = \\begin{pmatrix} \n",
    "2x_1 & 2 & 1 \\\\\n",
    "1 & 3x_2^2 & 2x_3 \\\\\n",
    "2 & 2x_2 & 3x_3^2 \n",
    "\\end{pmatrix}\n",
    "$$  \n",
    "\n",
    "各分量函数为分别为:  \n",
    "\n",
    "$$\n",
    "\\begin{cases} \n",
    "y_1 = x_1^2 + 2x_2 + x_3 \\\\\n",
    "y_2 = x_1 + x_2^3 + x_3^2 \\\\\n",
    "y_3 = 2x_1 + x_2^2 + x_3^3 \n",
    "\\end{cases}\n",
    "$$  \n",
    "\n",
    "投影方向：  \n",
    "\n",
    "$$\n",
    "v = (1, 1, 1)\n",
    "$$  \n",
    "\n",
    "$$\n",
    "v \\circ J = \\left[ 2x_1 + 1 + 2,\\, 2 + 3x_2^2 + 2x_2,\\, 1 + 2x_3 + 3x_3^2 \\right] = [5, 18, 34]\n",
    "$$  \n",
    "\n",
    "代码结果与分析相互印证  \n",
    "\n",
    "再来看看投影到不同的方向 $v = (3,2,1)$\n",
    "\n",
    "先分析\n",
    "\n",
    "$$\n",
    "v \\circ J = \\left[ 3 * 2x_1 + 2 * 1 + 1 * 2,\\, 3 * 2 + 2 * 3x_2^2 + 1 * 2x_2,\\, 3 * 1 + 2 * 2x_3 + 1 * 3x_3^2 \\right] = [10, 34, 42]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae07a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor(34.)\n",
      "tensor(42.)\n"
     ]
    }
   ],
   "source": [
    "# 再来看看投影到不同方向v =(3,2,1)\n",
    "x1 = torch.tensor(1,requires_grad=True,dtype=torch.float)\n",
    "x2 = torch.tensor(2,requires_grad=True,dtype=torch.float)\n",
    "x3 = torch.tensor(3,requires_grad=True,dtype=torch.float)\n",
    "y = torch.randn(3)\n",
    "y[0] = x1**2+2*x2+x3\n",
    "y[1] = x1+x2**3+x3**2\n",
    "y[2] = 2*x1+x2**2+x3**3\n",
    "v = torch.tensor([3,2,1],dtype=torch.float)\n",
    "y.backward(v)\n",
    "print(x1.grad)\n",
    "print(x2.grad)\n",
    "print(x3.grad) #依旧吻合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d20d89",
   "metadata": {},
   "source": [
    "总结如下\n",
    "- 如果 $v$ 是权重或向量函数的投影方向，它的大小必须与向量函数的个数对应\n",
    "- 如果最后的函数值是标量，则说明向量函数只有1个，`backward()`可以不传值，默认为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9b448f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 再来看看一些例子\n",
    "x = torch.tensor([1.0,2.0,3.0,4.0],requires_grad=True)\n",
    "y = 2*x\n",
    "z = y.view(2,2)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "997a651f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 0.2000, 0.0200, 0.0020])\n"
     ]
    }
   ],
   "source": [
    "#现在 y 不是⼀个标量，所以在调⽤ backward 时需要传⼊⼀个和 y 同形的权重向量进⾏加权求和得到⼀个标量\n",
    "v = torch.tensor([[1.0, 0.1], [0.01, 0.001]], dtype=torch.float)\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc6167",
   "metadata": {},
   "source": [
    "中断梯度追踪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c75d6955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0,requires_grad=True)\n",
    "y1 = x**2\n",
    "with torch.no_grad():\n",
    "    y2 = x**3\n",
    "y3 = y1+y2\n",
    "print(x.requires_grad)\n",
    "print(y1,y1.requires_grad)\n",
    "print(y2,y2.requires_grad)\n",
    "print(y3,y3.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd2bec1",
   "metadata": {},
   "source": [
    "这里的`y2`是没有`grad_fn`的，而`y3`是有的。\n",
    "\n",
    "那么将 `y3`对`x`求梯度的话是多少呢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "371fee21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "y3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af3a933",
   "metadata": {},
   "source": [
    "为什么是2呢？$y_3 = y_1 + y_2 = x^2 + x^3$，当 $x = 1$ 时 $\\frac{dy_3}{dx}$ 不应该是5吗？事实上，由于 $y_2$ 的定义是被 `torch.no_grad():` 包裹的，所以与 $y_2$ 有关的梯度是不会回传的，只有与 $y_1$ 有关的梯度才会回传，即 $x^2$ 对 $x$ 的梯度。\n",
    "\n",
    "既然不被追踪就不会影响最终的梯度，那么就可以这样：如果我们想要修改`tensor`的值，但是又不希望被`autograd`记录，那么我们可以对`tensor.data`进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b5472709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1,requires_grad=True)\n",
    "print(x.data)\n",
    "print(x.data.requires_grad) #还是tensor，但是默认的追踪为 False\n",
    "y1 = 2*x\n",
    "y2 = 3*x.data\n",
    "y3 = y1+y2\n",
    "y3.backward()\n",
    "print(x.grad) #这里随意更改x.data 的值都不会影响结果梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b532fd26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1aabf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
